{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sentense and related pos and chunking tag array\n",
    "def createSentense(filename):    \n",
    "    sentenses = list()\n",
    "    pos_sentenses = list()    \n",
    "    with open(filename, 'r') as ins:\n",
    "        sentense = list()\n",
    "        pos_sentense = list()        \n",
    "        for line in ins:\n",
    "            separator = line.split()            \n",
    "            if len(separator)==3:                \n",
    "                sentense.append(separator[0])\n",
    "                pos_sentense.append(separator[1])                \n",
    "            else:                \n",
    "                sentenses.append(sentense)\n",
    "                pos_sentenses.append(pos_sentense)                \n",
    "                sentense = list()\n",
    "                pos_sentense = list()                \n",
    "        sentenses.append(sentense)\n",
    "        pos_sentenses.append(pos_sentense)        \n",
    "    return sentenses,pos_sentenses\n",
    "\n",
    "train_sentenses,train_pos_sentenses = createSentense('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndividualList(sentenses,pos_sentenses):\n",
    "    words = list()\n",
    "    posTags = list()\n",
    "    for sentense in sentenses:\n",
    "        for word in sentense:\n",
    "            words.append(word)\n",
    "    for pos_sentense in pos_sentenses:\n",
    "        for posTag in pos_sentense:\n",
    "            posTags.append(posTag)\n",
    "    return words,posTags\n",
    "\n",
    "train_words,train_POS_tags = createIndividualList(train_sentenses,train_pos_sentenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#' '$' \"''\" '(' ')' ',' '.' ':' 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR'\n",
      " 'JJS' 'MD' 'NN' 'NNP' 'NNPS' 'NNS' 'PDT' 'POS' 'PRP' 'PRP$' 'RB' 'RBR'\n",
      " 'RBS' 'RP' 'SYM' 'TO' 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ' 'WDT' 'WP'\n",
      " 'WP$' 'WRB' '``']\n"
     ]
    }
   ],
   "source": [
    "# creating a dictionary of tags which will be used in creating onHotEncoding for each tag\n",
    "train_POS_tags_np = np.array(train_POS_tags)\n",
    "unique_tags = np.unique(train_POS_tags_np)\n",
    "index = np.argwhere(unique_tags=='-1')\n",
    "unique_tags = np.delete(unique_tags,index)\n",
    "print(unique_tags)\n",
    "\n",
    "# mapping each of the unique tags to an integer value\n",
    "tag_to_int = dict((c,i) for i,c in enumerate(unique_tags))\n",
    "int_to_tag = dict((i,c) for i,c in enumerate(unique_tags))\n",
    "\n",
    "# create oneHotEncoding for a particular tag\n",
    "def getOneHotTagEncoding(tag):    \n",
    "    resultset = [0] * len(unique_tags)\n",
    "    resultset[tag_to_int[tag]] = 1\n",
    "    return resultset\n",
    "\n",
    "#getOneHotTagEncoding('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a feature vector for a particular word\n",
    "def getWordFeatures(words,index):\n",
    "    features = list()\n",
    "    \n",
    "    #1 : Give high value if the word is to\n",
    "    if words[index] == 'to':\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    \n",
    "    #2 : if the word is DT\n",
    "    if words[index]=='a' or words[index]=='an' or words[index]=='the' or words[index]=='another' or words[index]=='both' or words[index]=='each':\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0.1)\n",
    "    \n",
    "    #3 : DT will always be followed by a Noun Phrase\n",
    "    if index>0 and words[index-1]=='a' or words[index-1]=='an' or words[index-1]=='the':\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0.5)\n",
    "    \n",
    "    #4 for comma\n",
    "    if words[index] == ',':\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    \n",
    "    #5 for full stop\n",
    "    if words[index] == '.':\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    \n",
    "    #6 for double quotes\n",
    "    if words[index] == '``':\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "    \n",
    "    #7 length of the word\n",
    "    features.append(len(words[index]))\n",
    "    \n",
    "    #8 if the word is not the first word in the sentense and its first letter is capitalizsed        \n",
    "    features.append(words[index][0].isupper())\n",
    "    \n",
    "    #9 ending in \"ing\"\n",
    "    features.append(words[index][-3:] == \"ing\")\n",
    "    \n",
    "    #10: ending in \"ly\"\n",
    "    features.append(words[index][-2:] == \"ly\")\n",
    "        \n",
    "    #11: contain a number\n",
    "    pattern = re.compile(r'\\d')\n",
    "    features.append(len(pattern.findall(words[index])) > 0)\n",
    "    \n",
    "    #12: hyphen\n",
    "    pattern = re.compile(r'-')\n",
    "    features.append(len(pattern.findall(words[index])) > 0)\n",
    "    \n",
    "    #13 if the previous word's pos tag is Adjective then there is high prob of current word being noun\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.1, 1, 0, 0, 0, 5, False, False, False, False, False, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# the feature vector is of the format: W-3 POS-3 W-2 POS-2 W-1 POS-1 W POS W+1 POS+1 W+2 POS+2 Chunkig_Tag-1\n",
    "def getFeatures(sentence,index,tags):\n",
    "    features = list()\n",
    "    wordFeature_len = [0]*12\n",
    "    posTagFeature_length = [0]*len(unique_tags)    \n",
    "                \n",
    "    # getting feature vector for current word\n",
    "    features.extend(getWordFeatures(sentence,index))    \n",
    "    \n",
    "    # getting feature vector for posTag-1\n",
    "    if index>=1:\n",
    "        features.extend(getOneHotTagEncoding(tags[index-1]))\n",
    "    else:        \n",
    "        features.extend(posTagFeature_length)\n",
    "    \n",
    "    return features\n",
    "\n",
    "features_found = getFeatures(['Confidence','in','the','pound','is','widely','expected'],3,['NN','IN','DT','NN','VBZ','RB','VBN'])\n",
    "feature_length = len(features_found)\n",
    "print(features_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Feature Matrix\n",
    "def getFeatureMatrix(train_sentenses,train_pos_sentenses):\n",
    "    resultset = list()\n",
    "    \n",
    "    size = len(train_sentenses)\n",
    "    i=0\n",
    "    while i<size:\n",
    "        sentence = train_sentenses[i]\n",
    "        posTags = train_pos_sentenses[i]    \n",
    "        if i==0:\n",
    "            print(sentence,posTags)\n",
    "        j=0\n",
    "        while j<len(sentence):\n",
    "            resultset.append(getFeatures(sentence,j,posTags))\n",
    "            #y.append(chunkingTags[j])\n",
    "            j=j+1\n",
    "        i = i+1\n",
    "        \n",
    "    return resultset\n",
    "\n",
    "def getLabelVector(posTags):\n",
    "    y = list()\n",
    "    for t in posTags:\n",
    "        y.append(tag_to_int[t])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Confidence', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', 'deficits', '.'] ['NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NNP', ',', 'JJ', 'IN', 'NN', 'NN', ',', 'VB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'POS', 'JJ', 'NNS', '.']\n"
     ]
    }
   ],
   "source": [
    "X = getFeatureMatrix(train_sentenses,train_pos_sentenses)\n",
    "y = getLabelVector(train_POS_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentenses,test_pos_sentenses = createSentense('test.txt')\n",
    "\n",
    "# Implementation of modified viterbi on the 'C'=44 (i.e, the count of tags)\n",
    "# Initialy trying to implement GREEDY MEMM Decoding\n",
    "def modifiedViterbi(sentenses):\n",
    "    n = len(sentenses)\n",
    "    posTags = list()\n",
    "    k=0\n",
    "    while k<2:\n",
    "        sentense = sentenses[k]        \n",
    "        length = len(sentense)\n",
    "        X_test = list()\n",
    "        i=0\n",
    "        while i<length:\n",
    "            X_test.append(getFeatures(sentense,i,posTags))\n",
    "        chunkingTags.append(logreg.predict(X_test))\n",
    "                \n",
    "    return posTags\n",
    "\n",
    "prediction = modifiedViterbi(test_sentenses)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
